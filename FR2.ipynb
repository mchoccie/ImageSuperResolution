{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29OYPK-pznE_"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage\n",
        "from google.colab.patches import cv2_imshow\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-k5VjwFzr9O",
        "outputId": "b4008e2c-ec74-4a1f-ff07-bc3bb97e3cb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlGkTumRomug"
      },
      "outputs": [],
      "source": [
        "def psnr(img1, img2):\n",
        "    mse = np.mean((img1 - img2) ** 2)\n",
        "    if mse == 0:\n",
        "        return float('inf')\n",
        "    max_pixel = 1.0\n",
        "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
        "    return psnr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STATE OF THE ART MODEL"
      ],
      "metadata": {
        "id": "Y3RjVnvUfBfL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz50LlGbzvA3"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class FSRCNN(nn.Module):\n",
        "    def __init__(self, scale_factor, num_channels=1, d=56, s=12, m=4):\n",
        "        super(FSRCNN, self).__init__()\n",
        "        self.first_part = nn.Sequential(\n",
        "            nn.Conv2d(num_channels, d, kernel_size=5, padding=5//2),\n",
        "            nn.PReLU(d)\n",
        "        )\n",
        "        self.mid_part = [nn.Conv2d(d, s, kernel_size=1), nn.PReLU(s)]\n",
        "        for _ in range(m):\n",
        "            self.mid_part.extend([nn.Conv2d(s, s, kernel_size=3, padding=3//2), nn.PReLU(s)])\n",
        "        self.mid_part.extend([nn.Conv2d(s, d, kernel_size=1), nn.PReLU(d)])\n",
        "        self.mid_part = nn.Sequential(*self.mid_part)\n",
        "        self.last_part = nn.ConvTranspose2d(d, num_channels, kernel_size=9, stride=scale_factor, padding=9//2,\n",
        "                                            output_padding=scale_factor-1)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.first_part:\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
        "                nn.init.zeros_(m.bias.data)\n",
        "        for m in self.mid_part:\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
        "                nn.init.zeros_(m.bias.data)\n",
        "        nn.init.normal_(self.last_part.weight.data, mean=0.0, std=0.001)\n",
        "        nn.init.zeros_(self.last_part.bias.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.first_part(x)\n",
        "        x = self.mid_part(x)\n",
        "        x = self.last_part(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA ACQUISITION AND PREPROCESSING"
      ],
      "metadata": {
        "id": "udoEboNHfF0B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQtHUOD30GTB"
      },
      "outputs": [],
      "source": [
        "class TrainDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, h5_file):\n",
        "        super(TrainDataset, self).__init__()\n",
        "        self.h5_file = h5_file\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        with h5py.File(self.h5_file, 'r') as f:\n",
        "            return np.expand_dims(f['lr'][idx] / 255., 0), np.expand_dims(f['hr'][idx] / 255., 0)\n",
        "\n",
        "    def __len__(self):\n",
        "        with h5py.File(self.h5_file, 'r') as f:\n",
        "            return len(f['lr'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWW6vopMG2fo"
      },
      "outputs": [],
      "source": [
        "class EvalDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, h5_file):\n",
        "        super(EvalDataset, self).__init__()\n",
        "        self.h5_file = h5_file\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        with h5py.File(self.h5_file, 'r') as f:\n",
        "            return np.expand_dims(f['lr'][str(idx)][:, :] / 255., 0), np.expand_dims(f['hr'][str(idx)][:, :] / 255., 0)\n",
        "\n",
        "    def __len__(self):\n",
        "        with h5py.File(self.h5_file, 'r') as f:\n",
        "            return len(f['lr'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfbdfTTf0Ipv",
        "outputId": "396c0a34-f2ae-49a0-9927-7a6ab7041797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3698808, 20, 20)\n",
            "Shape of the dataset: (3698808, 10, 10)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Replace 'yourfile.h5' with the path to your H5 file\n",
        "with h5py.File('/content/drive/MyDrive/FSRCNN/91-image_x2.h5', 'r') as f:\n",
        "    # Assuming 'image_data' is the key in your H5 file that contains images\n",
        "    dataset_shape = f['lr'].shape\n",
        "\n",
        "\n",
        "    print(f['hr'].shape)\n",
        "print(\"Shape of the dataset:\", dataset_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdjDbKzA2hsB"
      },
      "outputs": [],
      "source": [
        "train_dataset = TrainDataset('/content/drive/MyDrive/FSRCNN/91-image_x2_srcnn.h5')\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                                  batch_size=16,\n",
        "                                  shuffle=True,\n",
        "                                  pin_memory=True,\n",
        "                                  drop_last=True)\n",
        "# train_dataloader_iter = iter(train_dataloader)\n",
        "# batch_input_data, batch_target_labels = next(train_dataloader_iter)\n",
        "\n",
        "# # Print the shape of the target labels\n",
        "# print(batch_target_labels.shape)\n",
        "# print(batch_input_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J1cL-iuJjV7"
      },
      "outputs": [],
      "source": [
        "eval_dataset = EvalDataset('/content/drive/MyDrive/FSRCNN/Set5_x2.h5')\n",
        "eval_dataloader = DataLoader(dataset=eval_dataset, batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL ARCHITECTURE CHANGE AND FINE TUNING"
      ],
      "metadata": {
        "id": "OpdMND5tfLol"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KgQAp5yD0MNY"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "model = FSRCNN(1)\n",
        "epochs = 10\n",
        "batch_size = 10\n",
        "lr = 0.01\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.first_part.parameters()},\n",
        "    {'params': model.mid_part.parameters()},\n",
        "    {'params': model.last_part.parameters(), 'lr': lr * 0.1}\n",
        "], lr=lr)\n",
        "\n",
        "train_dataset = TrainDataset('/content/drive/MyDrive/FSRCNN/91-image_x2_srcnn.h5')\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=True,\n",
        "                                  pin_memory=True,\n",
        "                                  drop_last=True)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for lr_images, hr_images in tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
        "\n",
        "      # print(lr_images.shape)\n",
        "      # print(hr_images.shape)\n",
        "    # Zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = model(lr_images)\n",
        "      #print(outputs.shape)\n",
        "      # Compute loss\n",
        "      loss = criterion(outputs, hr_images)\n",
        "\n",
        "      # Backward pass and optimize\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "# Print epoch statistics\n",
        "print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_dataloader)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v__21-r00mv2"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/FSRCNN/Models/FSRCNN_weights_h5.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKCrCZbO0nV-",
        "outputId": "29114dc5-278d-4355-dc2a-9eceee58f382"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = FSRCNN(1)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/FSRCNN/Models/FSRCNN_weights_h5.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL EVALUATION"
      ],
      "metadata": {
        "id": "6tp1yMx6fSDg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgiDKz_XO4QV",
        "outputId": "1f9b7d13-7495-422a-e7cc-4c115edebb12"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.000311\n",
            "Average PSNR Difference: 2.303270\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "model.eval()\n",
        "\n",
        "# Criterion for loss\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Accumulators\n",
        "running_loss = 0.0\n",
        "psnr_score_loss = 0\n",
        "total_images = 0\n",
        "\n",
        "with torch.no_grad():  # Deactivate autograd for evaluation\n",
        "    for inputs, labels in tqdm(eval_dataloader, desc='Evaluating', leave=False):\n",
        "        # Forward pass: compute the model's predictions for the inputs\n",
        "        preds = model(inputs).clamp(0.0, 1.0)\n",
        "\n",
        "        loss = criterion(preds, labels)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        pred_i = preds.squeeze()\n",
        "        label_i = labels.squeeze()\n",
        "        input_i = inputs.squeeze()\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        pred_np = pred_i.cpu().numpy()\n",
        "        label_np = label_i.cpu().numpy()\n",
        "        input_np = input_i.cpu().numpy()\n",
        "\n",
        "        # Calculate PSNR for generated and original LR images\n",
        "        psnr_score_hr_gen = psnr(pred_np, label_np)\n",
        "        psnr_score_hr_lr = psnr(input_np, label_np)\n",
        "        psnr_score_loss += psnr_score_hr_gen - psnr_score_hr_lr\n",
        "        total_images += 1\n",
        "\n",
        "average_loss = running_loss / total_images\n",
        "average_psnr_diff = psnr_score_loss / total_images\n",
        "\n",
        "print(f'Test Loss: {average_loss:.6f}')\n",
        "print(f'Average PSNR Difference: {average_psnr_diff:.6f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INFERENCE, OPTIMIZATION AND REAL TIME TEST"
      ],
      "metadata": {
        "id": "MTmJu_w5fUrm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgkCbP6y0cqr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import PIL.Image as pil_image\n",
        "def convert_rgb_to_ycbcr(img):\n",
        "  if type(img) == np.ndarray:\n",
        "      y = 16. + (64.738 * img[:, :, 0] + 129.057 * img[:, :, 1] + 25.064 * img[:, :, 2]) / 256.\n",
        "      cb = 128. + (-37.945 * img[:, :, 0] - 74.494 * img[:, :, 1] + 112.439 * img[:, :, 2]) / 256.\n",
        "      cr = 128. + (112.439 * img[:, :, 0] - 94.154 * img[:, :, 1] - 18.285 * img[:, :, 2]) / 256.\n",
        "      return np.array([y, cb, cr]).transpose([1, 2, 0])\n",
        "  elif type(img) == torch.Tensor:\n",
        "      if len(img.shape) == 4:\n",
        "          img = img.squeeze(0)\n",
        "      y = 16. + (64.738 * img[0, :, :] + 129.057 * img[1, :, :] + 25.064 * img[2, :, :]) / 256.\n",
        "      cb = 128. + (-37.945 * img[0, :, :] - 74.494 * img[1, :, :] + 112.439 * img[2, :, :]) / 256.\n",
        "      cr = 128. + (112.439 * img[0, :, :] - 94.154 * img[1, :, :] - 18.285 * img[2, :, :]) / 256.\n",
        "      return torch.cat([y, cb, cr], 0).permute(1, 2, 0)\n",
        "  else:\n",
        "      raise Exception('Unknown Type', type(img))\n",
        "\n",
        "def convert_ycbcr_to_rgb(img):\n",
        "    if type(img) == np.ndarray:\n",
        "        r = 298.082 * img[:, :, 0] / 256. + 408.583 * img[:, :, 2] / 256. - 222.921\n",
        "        g = 298.082 * img[:, :, 0] / 256. - 100.291 * img[:, :, 1] / 256. - 208.120 * img[:, :, 2] / 256. + 135.576\n",
        "        b = 298.082 * img[:, :, 0] / 256. + 516.412 * img[:, :, 1] / 256. - 276.836\n",
        "        return np.array([r, g, b]).transpose([1, 2, 0])\n",
        "    elif type(img) == torch.Tensor:\n",
        "        if len(img.shape) == 4:\n",
        "            img = img.squeeze(0)\n",
        "        r = 298.082 * img[0, :, :] / 256. + 408.583 * img[2, :, :] / 256. - 222.921\n",
        "        g = 298.082 * img[0, :, :] / 256. - 100.291 * img[1, :, :] / 256. - 208.120 * img[2, :, :] / 256. + 135.576\n",
        "        b = 298.082 * img[0, :, :] / 256. + 516.412 * img[1, :, :] / 256. - 276.836\n",
        "        return torch.cat([r, g, b], 0).permute(1, 2, 0)\n",
        "    else:\n",
        "        raise Exception('Unknown Type', type(img))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rFzKXRR0q8S",
        "outputId": "b682b04f-4302-45fc-e3cc-b9531572a786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(256, 256, 3)\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "scale = 2\n",
        "image_file = '/content/drive/MyDrive/FSRCNN/butterfly_GT.bmp'\n",
        "hr_image_cv = cv2.imread(image_file)\n",
        "# image_file = '/content/drive/MyDrive/SRCNN/Train/t1.bmp'\n",
        "# hr_image_cv = cv2.imread(image_file)\n",
        "print(hr_image_cv.shape)\n",
        "hr_image_rgb = cv2.cvtColor(hr_image_cv, cv2.COLOR_BGR2RGB)\n",
        "hr_image_rgb = hr_image_rgb.astype('float32')/255.0\n",
        "image = pil_image.open(image_file).convert('RGB')\n",
        "\n",
        "image_width = (image.width // scale) * scale\n",
        "image_height = (image.height // scale) * scale\n",
        "image = image.resize((image_width, image_height), resample=pil_image.BICUBIC)\n",
        "image = image.resize((image.width // scale, image.height // scale), resample=pil_image.BICUBIC)\n",
        "image = image.resize((image.width * scale, image.height * scale), resample=pil_image.BICUBIC)\n",
        "image.save(image_file.replace('.', '_bicubic_x{}.'.format(scale)))\n",
        "\n",
        "image = np.array(image).astype(np.float32)\n",
        "imagepsnr = image/255.0\n",
        "ycbcr = convert_rgb_to_ycbcr(image)\n",
        "\n",
        "y = ycbcr[..., 0]\n",
        "y /= 255.\n",
        "y = torch.from_numpy(y)\n",
        "y = y.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = model(y).clamp(0.0, 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qz9pHGl8oACR"
      },
      "outputs": [],
      "source": [
        "preds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5xO_o8jlGpS"
      },
      "outputs": [],
      "source": [
        "print(preds.shape)\n",
        "preds = preds.mul(255.0).cpu().numpy().squeeze(0).squeeze(0)\n",
        "\n",
        "output = np.array([preds, ycbcr[..., 1], ycbcr[..., 2]]).transpose([1, 2, 0])\n",
        "output = np.clip(convert_ycbcr_to_rgb(output), 0.0, 255.0).astype(np.uint8)\n",
        "# output = output.astype('float32')/255.0\n",
        "# print(output)\n",
        "# print(output.shape)\n",
        "output = pil_image.fromarray(output)\n",
        "output.save(image_file.replace('.', '_srcnn_x{}.'.format(scale)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBhJFIgrlJhg"
      },
      "outputs": [],
      "source": [
        "output = pil_image.fromarray(output)\n",
        "output.save(image_file.replace('.', '_srcnn_x{}.'.format(scale)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbu7sSgsloCL"
      },
      "outputs": [],
      "source": [
        "def psnr(img1, img2):\n",
        "    mse = np.mean((img1 - img2) ** 2)\n",
        "    if mse == 0:\n",
        "        return float('inf')\n",
        "    max_pixel = 1.0\n",
        "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
        "    return psnr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWbiEIqElpz8",
        "outputId": "95742ac0-a0da-4a48-f26f-bd6c20883637"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Signal to noise ratio between generated image and HR image: 30.07604444085278\n",
            "Signal to noise ratio between LR image and HR image: 26.11647601222635\n"
          ]
        }
      ],
      "source": [
        "# print(output.shape)\n",
        "# print(image.shape)\n",
        "\n",
        "hr_image_cv = cv2.imread('/content/drive/MyDrive/FSRCNN/butterfly_GT.bmp')\n",
        "hr_image_rgb = cv2.cvtColor(hr_image_cv, cv2.COLOR_BGR2RGB)\n",
        "hr_image_rgb = hr_image_rgb.astype('float32')/255.0\n",
        "\n",
        "generated_image_cv = cv2.imread('/content/drive/MyDrive/FSRCNN/butterfly_GT_srcnn_x2.bmp')\n",
        "generated_image_rgb = cv2.cvtColor(generated_image_cv, cv2.COLOR_BGR2RGB)\n",
        "generated_image_rgb = generated_image_rgb.astype('float32')/255.0\n",
        "\n",
        "lr_image_cv = cv2.imread('/content/drive/MyDrive/FSRCNN/butterfly_GT_bicubic_x2.bmp')\n",
        "lr_image_rgb = cv2.cvtColor(lr_image_cv, cv2.COLOR_BGR2RGB)\n",
        "lr_image_rgb = lr_image_rgb.astype('float32')/255.0\n",
        "print(\"Signal to noise ratio between generated image and HR image: \" + str(psnr(hr_image_rgb, generated_image_rgb)))\n",
        "print(\"Signal to noise ratio between LR image and HR image: \" +str(psnr(hr_image_rgb, lr_image_rgb)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNeGMWb1xuLw",
        "outputId": "da48f3ea-ef47-4f6a-9712-93467eca9c53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 400, 400])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/jit/_script.py:1291: UserWarning: `optimize` is deprecated and has no effect. Use `with torch.jit.optimized_execution() instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "model = FSRCNN(1)\n",
        "lr_image_path = '/content/drive/MyDrive/SRCNN/Train_Set/Train_LR/t11.bmp'\n",
        "image = Image.open(lr_image_path).convert('RGB')\n",
        "image = np.array(image).astype(np.float32)\n",
        "ycbcr = convert_rgb_to_ycbcr(image)\n",
        "y = ycbcr[..., 0]\n",
        "y /= 255.\n",
        "y = torch.from_numpy(y)\n",
        "input_tensor = y.unsqueeze(0).unsqueeze(0)\n",
        "print(input_tensor.shape)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    # Add any other necessary preprocessing steps here\n",
        "])\n",
        "\n",
        "# Preprocess the image\n",
        "traced_model = torch.jit.script(model, input_tensor)\n",
        "traced_model.save(\"/content/drive/MyDrive/FSRCNN/Models/FSRCNN_weights_script_h5.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrEK-Tt6yq_V",
        "outputId": "bf5e91b7-c76c-45cf-ceb5-8374865c99ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 400, 400])\n",
            "0.6217529249191284\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "lr_image_path = '/content/drive/MyDrive/SRCNN/Train_Set/Train_LR/t11.bmp'\n",
        "model  = FSRCNN(1)\n",
        "loaded_model = torch.jit.load(\"/content/drive/MyDrive/FSRCNN/Models/FSRCNN_weights_script_h5.pt\")\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/FSRCNN/Models/FSRCNN_weights_h5.pth'))\n",
        "loaded_model.load_state_dict(model.state_dict())\n",
        "lr_image_path = '/content/drive/MyDrive/SRCNN/Train_Set/Train_LR/t11.bmp'\n",
        "image = Image.open(lr_image_path).convert('RGB')\n",
        "image = np.array(image).astype(np.float32)\n",
        "ycbcr = convert_rgb_to_ycbcr(image)\n",
        "y = ycbcr[..., 0]\n",
        "y /= 255.\n",
        "y = torch.from_numpy(y)\n",
        "input_tensor = y.unsqueeze(0).unsqueeze(0)\n",
        "print(input_tensor.shape)\n",
        "overall = 0\n",
        "for i in range(50):\n",
        "  start_time = time.time()\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output = loaded_model(input_tensor)\n",
        "  inference_time_raw = time.time() - start_time\n",
        "  overall += inference_time_raw\n",
        "print(overall/50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC5gDDEP10_H",
        "outputId": "4d9343d4-6258-4c1e-9fd4-b31fac95568f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 400, 400])\n",
            "0.6352539348602295\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "lr_image_path = '/content/drive/MyDrive/SRCNN/Train_Set/Train_LR/t11.bmp'\n",
        "model  = FSRCNN(1)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/FSRCNN/Models/FSRCNN_weights_h5.pth'))\n",
        "image = Image.open(lr_image_path).convert('RGB')\n",
        "image = np.array(image).astype(np.float32)\n",
        "ycbcr = convert_rgb_to_ycbcr(image)\n",
        "y = ycbcr[..., 0]\n",
        "y /= 255.\n",
        "y = torch.from_numpy(y)\n",
        "input_tensor = y.unsqueeze(0).unsqueeze(0)\n",
        "print(input_tensor.shape)\n",
        "overall = 0\n",
        "for i in range(50):\n",
        "  start_time = time.time()\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output = model(input_tensor)\n",
        "  inference_time_raw = time.time() - start_time\n",
        "  overall += inference_time_raw\n",
        "print(overall/50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}